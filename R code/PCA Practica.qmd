---
title: "PCA"
author: "Diego Isau Barranco Herrera"
format: pdf
editor: visual
---

```{r}
#| include: false
library(pacman)
p_load(psych, readxl, dplyr)
```

## Principal Component Analysis (PCA)

El PCA es una técnica de reducción de dimensión. A través de combinaciones lineales de las variables involucradas podemos reducir la cantidad de estas, siempre y cuando no se pierda información, esto es, tener como mínimo una varianza acumulada del 70% entre todos los componentes

## Población USA

Tenemos 19 variables independientes, vamos a separar por año para realizar el PCA

```{r}
poblacion <- read_xlsx("/var/home/diegob/Covid.xlsm")
```

### PCA para el año 2000

```{r}
poblacion_2000 <- poblacion %>% select(c(2,3,5,7,9,11,13,15,17,19))

# normalizando datos
poblacion_2000_normal <- scale(poblacion_2000)

# Correlación
det(cor(poblacion_2000_normal))  # La correlación tiende a cero

#psych::cor.plot(poblacion_2000_normal)

```

Ahora vamos a verificar el Factor de adecuación muestral de Kaiser, el cual nos indica si las variables son aptas para realizar el PCA

```{r}
#| warning: false
psych::KMO(poblacion_2000_normal)
```

Tenemos un MSA de 0.5, por lo que es una métrica útil y es pertinente hacer el PCA

Verificaremos cuántos componentes son adecuados para que tengamos mínimo una acumualción del 70% de la varianza

```{r}
pca_pob_2000 <- princomp(poblacion_2000_normal)
summary(pca_pob_2000)
```

Gráficamente

```{r}
#| eval: false
#| include: false
fviz_eig(pca_pob_2000, choice='variance')

fviz_eig(pca_pob_2000, choice ='eigenvalue')

```

Con dos componentes se acumula el 87% de la varianza

Puntuaciones factoriales

```{r}
# Grafico de las puntuaciones factoriales y su representación
fviz_pca_ind(poblacion_2000_normal,
             col.ind = 'cos2',
gradient.cols = c('red', 'yellow', 'green'),
             repel = FALSE)
```

Veamos cuánto contribuye cada variable a las diferentes componentes principales

```{r}
fviz_pca_var(pca, col.var = 'contrib', gradient.cols = c('red', 'yellow', 'green'), repel = FALSE)

fviz_pca_biplot(pca, col.var='red', col.ind = 'blue')
```

Haciendo el PCA con

### PCA para el año 2001

Se realzará el mismo procecimiento que en el año 2000 y al final se darán las interpretaciones

```{r}
poblacion_2001 <- poblacion %>% select(c(4,6,8,10,12,14,16,18,20))

# normalizando datos
poblacion_2001_normal <- scale(poblacion_2001)

# Correlación
det(cor(poblacion_2001_normal))  # La correlación tiende a cero

#psych::cor.plot(poblacion_2000_normal)
```

```{r}
#| message: false
#| warning: false
psych::KMO(poblacion_2000_normal)
```

```{r}
pca_pob_2001 <- princomp(poblacion_2001_normal)
summary(pca_pob_2001)
```

```{r}
#| eval: false
#| include: false
fviz_eig(pca_pob_2001, choice='variance')

fviz_eig(pca_pob_2001, choice ='eigenvalue')
```

```{r}
# Grafico de las puntuaciones factoriales y su representación
fviz_pca_ind(poblacion_2000_normal,
             col.ind = 'cos2',
gradient.cols = c('red', 'yellow', 'green'),
             repel = FALSE)


```

```{r}
# cuanto contribuye a cada variable
fviz_pca_var(pca, col.var = 'contrib', gradient.cols = c('red', 'yellow', 'green'), repel = FALSE)

fviz_pca_biplot(pca, col.var='red', col.ind = 'blue')
```

# data_pca

```{r}
data_pca <- read.csv2("data_pca.csv")

# normalizar datos
data_normal <- scale(data_pca[,-16])
View(data_normal)

```

Veamos las estadísticas básicas para determinar si es viable realizar el PCA

```{r}
# determinante de correlación
det(cor(data_normal))  # La correlación tiende a cero
psych::cor.plot(data_normal)

# Factor de adecuación muestral de kaiser
psych::KMO(data_normal)  #

```

Tenemos un MSA de 0.34, es muy bajo por lo que no se recomienda PCA, sin embargo procedemos con el ejercicio

```{r}
pca <- princomp(data_normal)
summary(pca)

```

Vamos a considerar 6 componentes, donde hay una acumulación de varianza del 70%, como se puede apreciar en las siguientes gráficas

```{r}
# grafica de eigenvalores y varianza
fviz_eig(pca, choice='variance')

fviz_eig(pca, choice ='eigenvalue')


```

Sus puntuaciones factoriales

```{r}
# Grafico de las puntuaciones factoriales y su representación
fviz_pca_ind(pca,
             col.ind = 'cos2',
             gradient.cols = c('red', 'yellow', 'green'),
             repel = FALSE)  # Existe una gran cantidad de datos no bien representados correctamente
```

Realizamos el pca con el número de componentes seguridos

```{r}
# PCA con 6 componentes

pca_6comp <- psych::principal(data_normal, nfactors = 6, residuals=FALSE, rotate="varimax", scores=TRUE, oblique.scores=FALSE, method='regression', use='pairwise', cor='cor', weight=NULL)
pca_6comp$weights

# Nuevas variables obtenidas, cuya principal caracteriatistica es que son ortogonales, es decir linealmente indNuevas variables obtenidas, cuya principal caracteriatistica es que son ortogonales, es decir linealmente independientesependientes

# Por lo anterior un conjunto de 15 variables altamente relacionadas se redujo unicamente dos variables cuya característica es que son ortogonales
```
